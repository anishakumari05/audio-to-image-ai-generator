{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf842c9-afb8-4af0-975a-4ef21ccb838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, pipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa41a646-0de9-4627-81cb-09f89045de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689e7d58-69aa-4fa3-becf-999d32a002e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_stemming(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by applying stemming.\n",
    "    Parameters:\n",
    "    - text (str): Input text for stemming.\n",
    "    \n",
    "    Returns:\n",
    "    - stemmed_text (str): Text after stemming.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2bf99c5-f1b1-4812-b3d3-944ecd3f2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text, sentiment_pipeline):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of the given text.\n",
    "    Parameters:\n",
    "    - text (str): Input text for sentiment analysis.\n",
    "    - sentiment_pipeline (pipeline): Hugging Face sentiment analysis pipeline.\n",
    "    \n",
    "    Returns:\n",
    "    - sentiment (str): Sentiment label (e.g., \"positive\", \"neutral\", \"negative\").\n",
    "    \"\"\"\n",
    "    sentiment_result = sentiment_pipeline(text)\n",
    "    sentiment = sentiment_result[0][\"label\"]\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e023fee-39b2-4a2b-97d4-ae885f645bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_to_image_with_dataset(\n",
    "    dataset_path,\n",
    "    whisper_model_path,\n",
    "    sd_model_path,\n",
    "    audio_files_directory\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes an audio dataset to generate transcriptions, analyze sentiments, and optionally create images.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_path (str): Path to the dataset CSV file containing 'File_name' and 'phrase' columns.\n",
    "    - whisper_model_path (str): Path to the fine-tuned Whisper ASR model.\n",
    "    - sd_model_path (str): Path or Hugging Face model ID for the Stable Diffusion model.\n",
    "    - audio_files_directory (str): Directory where audio files are located.\n",
    "    \n",
    "    Returns:\n",
    "    - results (list of dicts): A list of results containing transcription, sentiment, and optionally generated images.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(dataset_path)  # Ensure the dataset is structured correctly\n",
    "\n",
    "    # Step 1: Load the fine-tuned Whisper model using Wav2Vec2ForCTC and Wav2Vec2Processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(whisper_model_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(whisper_model_path)\n",
    "    \n",
    "    # Step 2: Load the sentiment analysis model\n",
    "    sentiment_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"  # Pre-trained model\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=sentiment_model_name)\n",
    "    \n",
    "    # Step 3: Load the Stable Diffusion model\n",
    "    sd_pipeline = StableDiffusionPipeline.from_pretrained(sd_model_path, torch_dtype=torch.float32)  # CPU mode\n",
    "    sd_pipeline.to(\"cpu\")  # Ensure it runs on CPU\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        audio_file_name = row['File_name']  # Get the file name directly from the dataset\n",
    "        \n",
    "        # Construct the full audio file path using the directory\n",
    "        audio_file_path = os.path.join(audio_files_directory, audio_file_name)\n",
    "        \n",
    "        # Check if the file exists before processing\n",
    "        if os.path.exists(audio_file_path):\n",
    "            # Transcribe the audio\n",
    "            audio_data, _ = librosa.load(audio_file_path, sr=16000)  # Load audio file with librosa\n",
    "            input_values = processor(audio_data, return_tensors=\"pt\").input_values  # Preprocess audio for the model\n",
    "            logits = model(input_values).logits  # Get model predictions\n",
    "            \n",
    "            # Detach the tensor to avoid gradient tracking and convert to NumPy\n",
    "            transcription_logits = logits.detach().numpy()\n",
    "            transcription = processor.decode(transcription_logits[0], skip_special_tokens=True)  # Decode the logits to text\n",
    "            \n",
    "            # Analyze sentiment\n",
    "            sentiment = analyze_sentiment(preprocessed_text, sentiment_pipeline)\n",
    "\n",
    "            # Generate image if sentiment is not negative\n",
    "            if \"negative\" not in sentiment.lower():\n",
    "                generated_image = sd_pipeline(preprocessed_text).images[0]\n",
    "            else:\n",
    "                generated_image = None\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"File_name\": audio_file_name,\n",
    "                \"Transcription\": transcription,\n",
    "                \"Preprocessed_Text\": preprocessed_text,\n",
    "                \"Sentiment\": sentiment,\n",
    "                \"Generated_Image\": generated_image\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Audio file not found: {audio_file_path}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b6afab-324c-430a-836f-3c574cce86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:/Users/HP/OneDrive/Desktop/Infosys Springboard/Dataset/Recordings/audio__details.csv\"\n",
    "whisper_model_path = \"C:/Users/HP/OneDrive/Desktop/Infosys Springboard/whisper-finetuned\"\n",
    "sd_model_path = \"stabilityai/stable-diffusion-2-1\"\n",
    "audio_files_directory = \"C:/Users/HP/OneDrive/Desktop/Infosys Springboard/Dataset/Recordings/Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026d5471-49a5-4784-a787-7b1d76347354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type whisper to instantiate a model of type wav2vec2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at C:/Users/HP/OneDrive/Desktop/Infosys Springboard/whisper-finetuned and are newly initialized: ['encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.attention.k_proj.bias', 'encoder.layers.0.attention.k_proj.weight', 'encoder.layers.0.attention.out_proj.bias', 'encoder.layers.0.attention.out_proj.weight', 'encoder.layers.0.attention.q_proj.bias', 'encoder.layers.0.attention.q_proj.weight', 'encoder.layers.0.attention.v_proj.bias', 'encoder.layers.0.attention.v_proj.weight', 'encoder.layers.0.feed_forward.intermediate_dense.bias', 'encoder.layers.0.feed_forward.intermediate_dense.weight', 'encoder.layers.0.feed_forward.output_dense.bias', 'encoder.layers.0.feed_forward.output_dense.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.1.attention.k_proj.bias', 'encoder.layers.1.attention.k_proj.weight', 'encoder.layers.1.attention.out_proj.bias', 'encoder.layers.1.attention.out_proj.weight', 'encoder.layers.1.attention.q_proj.bias', 'encoder.layers.1.attention.q_proj.weight', 'encoder.layers.1.attention.v_proj.bias', 'encoder.layers.1.attention.v_proj.weight', 'encoder.layers.1.feed_forward.intermediate_dense.bias', 'encoder.layers.1.feed_forward.intermediate_dense.weight', 'encoder.layers.1.feed_forward.output_dense.bias', 'encoder.layers.1.feed_forward.output_dense.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.2.attention.k_proj.bias', 'encoder.layers.2.attention.k_proj.weight', 'encoder.layers.2.attention.out_proj.bias', 'encoder.layers.2.attention.out_proj.weight', 'encoder.layers.2.attention.q_proj.bias', 'encoder.layers.2.attention.q_proj.weight', 'encoder.layers.2.attention.v_proj.bias', 'encoder.layers.2.attention.v_proj.weight', 'encoder.layers.2.feed_forward.intermediate_dense.bias', 'encoder.layers.2.feed_forward.intermediate_dense.weight', 'encoder.layers.2.feed_forward.output_dense.bias', 'encoder.layers.2.feed_forward.output_dense.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.3.attention.k_proj.bias', 'encoder.layers.3.attention.k_proj.weight', 'encoder.layers.3.attention.out_proj.bias', 'encoder.layers.3.attention.out_proj.weight', 'encoder.layers.3.attention.q_proj.bias', 'encoder.layers.3.attention.q_proj.weight', 'encoder.layers.3.attention.v_proj.bias', 'encoder.layers.3.attention.v_proj.weight', 'encoder.layers.3.feed_forward.intermediate_dense.bias', 'encoder.layers.3.feed_forward.intermediate_dense.weight', 'encoder.layers.3.feed_forward.output_dense.bias', 'encoder.layers.3.feed_forward.output_dense.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.4.attention.k_proj.bias', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.4.attention.out_proj.bias', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.4.attention.q_proj.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.pos_conv_embed.conv.bias', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'feature_extractor.conv_layers.0.conv.weight', 'feature_extractor.conv_layers.0.layer_norm.bias', 'feature_extractor.conv_layers.0.layer_norm.weight', 'feature_extractor.conv_layers.1.conv.weight', 'feature_extractor.conv_layers.2.conv.weight', 'feature_extractor.conv_layers.3.conv.weight', 'feature_extractor.conv_layers.4.conv.weight', 'feature_extractor.conv_layers.5.conv.weight', 'feature_extractor.conv_layers.6.conv.weight', 'feature_projection.layer_norm.bias', 'feature_projection.layer_norm.weight', 'feature_projection.projection.bias', 'feature_projection.projection.weight', 'lm_head.bias', 'lm_head.weight', 'masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96589e1365324fce9d62cf92ad268f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_audio_to_image_with_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhisper_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msd_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_files_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[1;32mIn[5], line 51\u001b[0m, in \u001b[0;36mprocess_audio_to_image_with_dataset\u001b[1;34m(dataset_path, whisper_model_path, sd_model_path, audio_files_directory)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Detach the tensor to avoid gradient tracking and convert to NumPy\u001b[39;00m\n\u001b[0;32m     50\u001b[0m transcription_logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 51\u001b[0m transcription \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscription_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Decode the logits to text\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Analyze sentiment\u001b[39;00m\n\u001b[0;32m     54\u001b[0m sentiment \u001b[38;5;241m=\u001b[39m analyze_sentiment(preprocessed_text, sentiment_pipeline)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:166\u001b[0m, in \u001b[0;36mWav2Vec2Processor.decode\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    to the docstring of this method for more information.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\whisper\\tokenization_whisper_fast.py:354\u001b[0m, in \u001b[0;36mWhisperTokenizerFast.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, output_offsets, time_precision, decode_with_timestamps, normalize, basic_normalize, remove_diacritics, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    349\u001b[0m filtered_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_token_ids(\n\u001b[0;32m    350\u001b[0m     token_ids,\n\u001b[0;32m    351\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m    352\u001b[0m )\n\u001b[1;32m--> 354\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbasic_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasic_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_diacritics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_diacritics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decode_with_timestamps:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# legacy method to decode timestamps when not included in the tokenizer vocabulary\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_with_timestamps(\n\u001b[0;32m    366\u001b[0m         filtered_ids, time_precision\u001b[38;5;241m=\u001b[39mtime_precision, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens\n\u001b[0;32m    367\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4004\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   4002\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 4004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4008\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4009\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\whisper\\tokenization_whisper_fast.py:380\u001b[0m, in \u001b[0;36mWhisperTokenizerFast._decode\u001b[1;34m(self, normalize, basic_normalize, remove_diacritics, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, basic_normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, remove_diacritics: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 380\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    383\u001b[0m         clean_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    653\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[1;32m--> 654\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    657\u001b[0m     clean_up_tokenization_spaces\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[0;32m    660\u001b[0m )\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = process_audio_to_image_with_dataset(dataset_path, whisper_model_path, sd_model_path, audio_files_directory)\n",
    "\n",
    "    # Display results\n",
    "    for result in results:\n",
    "        print(f\"File: {result['File_name']}\")\n",
    "        print(f\"Transcription: {result['Transcription']}\")\n",
    "        print(f\"Preprocessed Text: {result['Preprocessed_Text']}\")\n",
    "        print(f\"Sentiment: {result['Sentiment']}\")\n",
    "        if result['Generated_Image']:\n",
    "            result['Generated_Image'].show()  # Display the generated image\n",
    "        else:\n",
    "            print(\"Image generation skipped due to negative sentiment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba8f51-c95a-4fe2-93a3-5b900b3f212f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
